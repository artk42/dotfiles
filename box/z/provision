#!/usr/bin/env bash

set -euxo pipefail

# Inspect existing disks
lsblk

# Undo existing setups to allow running the script multiple times to iterate on it.
# We allow these operations to fail for the case the script runs the first time.
set +e
umount /mnt/boot/esp*
umount /mnt
vgchange -an
set -e

# Stop all mdadm arrays that the boot may have activated.
mdadm --stop --scan

# Create partition tables
parted --script /dev/sda mklabel gpt
parted --script /dev/sdb mklabel gpt

# Create partitions
parted --script --align optimal /dev/sda -- mklabel gpt mkpart esp0 fat32 1MB 551MB set 1 esp on mkpart array0 551MB 100%
parted --script --align optimal /dev/sdb -- mklabel gpt mkpart esp1 fat32 1MB 551MB set 1 esp on mkpart array1 551MB 100%

# Relaod partitions
partprobe

# Wait for all devices to exist
udevadm settle --timeout=5 --exit-if-exists=/dev/disk/by-partlabel/esp0
udevadm settle --timeout=5 --exit-if-exists=/dev/disk/by-partlabel/esp1
udevadm settle --timeout=5 --exit-if-exists=/dev/disk/by-partlabel/array0
udevadm settle --timeout=5 --exit-if-exists=/dev/disk/by-partlabel/array1

# Wipe RAID signatures
mdadm --zero-superblock /dev/disk/by-partlabel/array0
mdadm --zero-superblock /dev/disk/by-partlabel/array1

# Create RAIDs
mdadm --create --run --verbose /dev/md/array --level=1 --raid-devices=2 --homehost=z --name=array /dev/disk/by-partlabel/array0 /dev/disk/by-partlabel/array1

# Assembling the RAID can result in auto-activation of previously-existing LVM
# groups, preventing the RAID block device wiping below with
# `Device or resource busy`. So disable all VGs first.
vgchange -an

# Wipe filesystem signatures that might be on the RAID from some
# possibly existing older use of the disks (RAID creation does not do that).
# See https://serverfault.com/questions/911370/why-does-mdadm-zero-superblock-preserve-file-system-information
wipefs -a /dev/md/array

# Disable RAID recovery. We don't want this to slow down machine provisioning
# in the rescue mode. It can run in normal operation after reboot.
echo 0 > /proc/sys/dev/raid/speed_limit_max

# LVM
# PVs
pvcreate /dev/md/array
# VGs
vgcreate array /dev/md/array
# LVs
lvcreate --extents 100%FREE -n root array

# Filesystems
mkfs.fat -F 32 -n esp0 /dev/disk/by-partlabel/esp0
mkfs.fat -F 32 -n esp1 /dev/disk/by-partlabel/esp1
mkfs.ext4 -F -L root /dev/mapper/array-root

# Trigger udev so that the entries in /dev/disk/by-uuid get refreshed.
udevadm trigger

# Wait for FS labels to appear
udevadm settle --timeout=5 --exit-if-exists=/dev/disk/by-label/root

# NixOS pre-installation mounts

# Mount target root partition
mount /dev/disk/by-label/root /mnt
# Mount efivars unless already mounted
# (OVH rescue doesn't have them by default and the NixOS installer needs this)
mount | grep efivars || mount -t efivarfs efivarfs /sys/firmware/efi/efivars
# Mount our ESP partitions
mkdir -p /mnt/boot/esp0
mkdir -p /mnt/boot/esp1
mount /dev/disk/by-label/esp0 /mnt/boot/esp0
mount /dev/disk/by-label/esp1 /mnt/boot/esp1

# Installing nix

# Allow installing nix as root
mkdir -p /etc/nix
echo "build-users-group =" > /etc/nix/nix.conf

curl -L https://nixos.org/nix/install | sh
set +u +x # sourcing this may refer to unset variables that we have no control over
. $HOME/.nix-profile/etc/profile.d/nix.sh
set -u -x

nix-channel --add https://nixos.org/channels/nixos-21.05 nixos
nix-channel --update

# Getting NixOS installation tools
nix-env -iE "_: with import <nixpkgs/nixos> { configuration = {}; }; with config.system.build; [ nixos-generate-config nixos-install nixos-enter manual.manpages ]"

nixos-generate-config --root /mnt

# Copy `configuration.nix`
cp ~/configuration.nix /mnt/etc/nixos/configuration.nix

# Install NixOS
`which nixos-install` --no-root-passwd --root /mnt --max-jobs 40
